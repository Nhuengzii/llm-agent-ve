{
    "llm_api": [
        {
            "label": "gpt-3.5-turbo",
            "name": "GPT-3.5 Turbo",
            "description": "OpenAI's most capable GPT-3.5 model, optimized for chat interactions with improved performance and cost-effectiveness."
        },
        {
            "label": "gpt-4",
            "name": "GPT-4",
            "description": "OpenAI's most advanced system, capable of handling complex instructions and generating creative responses with high accuracy."
        },
        {
            "label": "gpt-4o",
            "name": "GPT-4o",
            "description": "(If it exists) A potential variant of GPT-4, likely tailored for specific use cases or with additional features. (Not officially confirmed by OpenAI at this time)."
        },
        {
            "label": "claude-3-haiku-20240307",
            "name": "Claude 3 Haiku",
            "description": "Anthropic's Claude 3 model specialized for generating haiku poetry, with a focus on creativity and adherence to the traditional format."
        },
        {
            "label": "claude-3-5-sonnet-20240620",
            "name": "Claude 3.5 Sonnet",
            "description": "Anthropic's Claude 3.5 model fine-tuned for crafting sonnets, showcasing its ability to produce structured and eloquent poetry."
        },
        {
            "label": "claude-3-opus-20240229",
            "name": "Claude 3 Opus",
            "description": "Anthropic's Claude 3 model with a broader range of capabilities, suitable for various tasks requiring creative and informative text generation."
        },
        {
            "label": "llama3-8b-8192",
            "name": "Groq LLaMa3 8B",
            "description": "Groq's optimized version of the LLaMa3 8 billion parameter model, known for its efficiency and performance on Groq hardware."
        },
        {
            "label": "llama3-70b-8192",
            "name": "Groq LLaMa3 70B",
            "description": "Groq's optimized version of the LLaMa3 70 billion parameter model, designed for complex tasks on Groq hardware."
        },
        {
            "label": "mixtral-8x7b-32768",
            "name": "Groq Mixtral 7B",
            "description": "Groq's model based on the Mixtral architecture, which may offer a blend of different approaches to language understanding and generation."
        },
        {
            "label": "whisper-large-v3",
            "name": "Groq Whisper Large V3",
            "description": "Groq's version of OpenAI's Whisper Large V3 model, specialized for speech recognition tasks."
        }
    ],
    "llm_local": [
        {
            "name": "deepseek_coder_v2",
            "label": "deepseek-coder-v2",
            "description": "Code generation model for Deepseek Coder V2"
        },
        {
            "name": "qwen2_72b",
            "label": "qwen2:72b",
            "description": "Qwen2 chatbot model with 72 billion parameters"
        },
        {
            "name": "llama3_70b_instruct",
            "label": "llama3:70b-instruct",
            "description": "Instruction-tuned LLaMa3 model with 70 billion parameters"
        },
        {
            "name": "llama3_70b",
            "label": "llama3:70b",
            "description": "LLama3 model with 70 billion parameters"
        },
        {
            "name": "llama3",
            "label": "llama3",
            "description": "Base LLaMa3 model"
        },
        {
            "name": "chat_llama3_70b_instruct",
            "label": "llama3:70b-instruct",
            "description": "Chat version of instruction-tuned LLaMa3 70B"
        },
        {
            "name": "chat_llama3",
            "label": "llama3",
            "description": "Chat version of base LLaMa3 model"
        },
        {
            "name": "chat_llama3_8b_1M_cw",
            "label": "llama3-gradient:8b",
            "description": "Chat LLaMa3 8B with gradient optimization and 1 million token context window"
        },
        {
            "name": "chat_llama3_70b_1M_cw",
            "label": "llama3-gradient:70b",
            "description": "Chat LLaMa3 70B with gradient optimization and 1 million token context window"
        },
        {
            "name": "chat_llama3_chatqa_8b",
            "label": "llama3-chatqa:8b",
            "description": "Chat LLaMa3 model for question answering with 8 billion parameters"
        },
        {
            "name": "chat_llama3_chatqa_70b",
            "label": "llama3-chatqa:70b",
            "description": "Chat LLaMa3 model for question answering with 70 billion parameters"
        },
        {
            "name": "qwen2_72b",
            "label": "qwen2:72b",
            "description": "Chat version of Qwen2 with 72 billion parameters"
        },
        {
            "name": "qwen2_15b",
            "label": "qwen2:1.5b",
            "description": "Qwen2 chatbot model with 1.5 billion parameters"
        },
        {
            "name": "qwen2_05b",
            "label": "qwen2:0.5b",
            "description": "Qwen2 chatbot model with 0.5 billion parameters"
        },
        {
            "name": "mistral",
            "label": "mistral",
            "description": "Chat model based on Mistral architecture"
        },
        {
            "name": "mixtral_8b",
            "label": "mixtral:8x7b",
            "description": "Chat model based on Mixtral architecture with 8x7 billion parameters"
        },
        {
            "name": "phi_mini_8b",
            "label": "phi3:3.8b",
            "description": "Phi 3 chatbot model with 3.8 billion parameters"
        },
        {
            "name": "phi_mid_14b",
            "label": "phi3:14b",
            "description": "Phi 3 chatbot model with 14 billion parameters"
        },
        {
            "name": "phi_mid_128k_14b",
            "label": "phi3:medium-128k",
            "description": "Phi 3 chatbot model with medium-128k context window and 14 billion parameters"
        },
        {
            "name": "gemma2_9b",
            "label": "gemma2:9b",
            "description": "Gemma2 chatbot model with 9 billion parameters"
        },
        {
            "name": "gemma2_27b",
            "label": "gemma2:27b",
            "description": "Gemma2 chatbot model with 27 billion parameters"
        },
        {
            "name": "aya_8b",
            "label": "aya:8b",
            "description": "Aya chatbot model with 8 billion parameters"
        },
        {
            "name": "aya_35b",
            "label": "aya:35b",
            "description": "Aya chatbot model with 35 billion parameters"
        },
        {
            "name": "deepseek_coder_v2",
            "label": "deepseek-coder-v2",
            "description": "Chat version of Deepseek Coder V2 code generation model"
        },
        {
            "name": "granite_code_8b",
            "label": "granite-code:8b",
            "description": "Granite code model with 8 billion parameters"
        },
        {
            "name": "codestral",
            "label": "codestral",
            "description": "Code generation model based on Codestral architecture"
        },
        {
            "name": "starcoder2_3b",
            "label": "starcoder2:3b",
            "description": "Starcoder2 code generation model with 3 billion parameters"
        },
        {
            "name": "starcoder2_15b",
            "label": "starcoder2:15b",
            "description": "Starcoder2 code generation model with 15 billion parameters"
        },
        {
            "name": "codegemma_7b",
            "label": "codegemma:7b",
            "description": "Codegemma code generation model with 7 billion parameters"
        },
        {
            "name": "internlm2",
            "label": "internlm2",
            "description": "Chatbot model based on InternLM2 architecture"
        },
        {
            "name": "dbrx_123b",
            "label": "dbrx",
            "description": "DBRX chatbot model with 123 billion parameters"
        }
    ],
    "llm_huggingface": [
        {
            "name": "Qwen2-0.5B-Instruct",
            "label": "Qwen2-0.5B-Instruct",
            "description": "Qwen2 chatbot model with 0.5 billion parameters, fine-tuned for instruction-based tasks."
        },
        {
            "name": "Qwen2-1.5B-Instruct",
            "label": "Qwen2-1.5B-Instruct",
            "description": "Qwen2 chatbot model with 1.5 billion parameters, fine-tuned for instruction-based tasks."
        }
    ]
}
